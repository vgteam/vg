# The VG tests are going to use toil-vg.
# toil-vg needs to be able to mount paths it can see into Docker containers.
# There's no good way to do that when running Docker containers as siblings of a container containing toil-vg.
# So we either have to genuinely nest Docker inside another Docker, or we have to run the build on the real host.

# Pull in an image that has our apt packages pre-installed, to save time installing them for every test.
# Make sure to use :latest so we re-check and re-pull if needed on every run.
image: quay.io/vgteam/vg_ci_prebake:latest

before_script:
  - sudo apt-get -q -y update
  # Make sure we have some curl stuff for pycurl which we need for some Python stuff
  # And the CI report upload needs uuidgen from uuid-runtime
  - sudo apt-get -q -y install --no-upgrade docker.io python3-pip python3-virtualenv libcurl4-gnutls-dev python-dev npm nodejs node-gyp uuid-runtime libgnutls28-dev doxygen
  - which junit-merge || sudo npm install -g junit-merge
  # Configure Docker to use a mirror for Docker Hub and restart the daemon
  - |
    if [[ ! -z "${DOCKER_HUB_MIRROR}" ]] ; then
      if [[ "${DOCKER_HUB_MIRROR}" == https* ]] ; then
        # Set up a secure mirror
        echo "{\"registry-mirrors\": [\"${DOCKER_HUB_MIRROR}\", \"https://mirror.gcr.io\"]}\"]}" | sudo tee /etc/docker/daemon.json
      else
        # Set up an insecure mirror
        echo "{\"registry-mirrors\": [\"${DOCKER_HUB_MIRROR}\", \"https://mirror.gcr.io\"], \"insecure-registries\": [\"${DOCKER_HUB_MIRROR##*://}\"]}" | sudo tee /etc/docker/daemon.json
      fi
    fi
  - startdocker || true
  - docker info
  - mkdir -p ~/.aws && cp "$GITLAB_SECRET_FILE_AWS_CREDENTIALS" ~/.aws/credentials

after_script:
  - stopdocker || true
  
# We have two pipeline stages: build to make a Docker, and test to run tests.
# TODO: make test stage parallel
stages:
  - build
  - test
  - report
  
# We define one job to do the out-of-container (re)build, and run the Bash
# tests. It uses a Gitlab-managed cache to prevent a full rebuild every time. It
# still takes longer than the Docker build, so we put it in the test stage
# alongside other longer jobs.
local-build-test-job:
  stage: test
  cache:
    # Gitlab isn't clever enough to fill PR caches from the main branch, so we
    # just use one megacache and hope the Makefile is smart enough to recover
    # from the future
    key: local-build-test-cache
    paths:
      - deps/
      - include/
      - lib/
      - bin/
      - obj/
  before_script:
    - sudo apt-get -q -y update
    # We need to make sure we get the right submodule files for this version
    # and don't clobber sources with the cache. We want to have a dirty state
    # with the correct source files.
    - scripts/restore-deps.sh
  script:
    - python3 ./configure.py
    - source ./source_me.sh
    - make get-deps
    - make -j8
    - echo Testing
    - bin/vg test "Target to alignment extraction"
    - echo Full Testing
    - make test
    - make static -j8
    # Also test as a backend for the tube map
    - git clone https://github.com/vgteam/sequenceTubeMap.git
    - cd sequenceTubeMap
    # Tube map expects local IPv6 but Kubernetes won't let us have it
    - 'sed -i "s/^}$/,\"serverBindAddress\": \"127.0.0.1\"}/" src/config.json'
    - npm install
    - CI=true npm run test
  variables:
    VG_FULL_TRACEBACK: "1"
    GIT_SUBMODULE_STRATEGY: none


# We define one job to do the Docker container build
build-job:
  stage: build
  before_script:
    # Don't bother starting the Docker daemon or installing much
    - which docker || (sudo apt-get -q -y update && sudo apt-get -q -y install --no-upgrade docker.io)
    # Get buildx
    - mkdir -p ~/.docker/cli-plugins/ ; curl -L https://github.com/docker/buildx/releases/download/v0.6.3/buildx-v0.6.3.linux-amd64 >  ~/.docker/cli-plugins/docker-buildx ; chmod u+x ~/.docker/cli-plugins/docker-buildx
    # Connect to the Kubernetes-based builder "buildkit"
    # See vgci/buildkit-deployment.yml
    - docker buildx create --use --name=buildkit --platform=linux/amd64,linux/arm64 --node=buildkit-amd64 --driver=kubernetes --driver-opt="nodeselector=kubernetes.io/arch=amd64"
    # Report on the builders
    - docker buildx inspect --bootstrap || true
    # Prune down build cache to make space
    - (echo "y" | docker buildx prune --keep-storage 80G) || true
  script:
    - make include/vg_git_version.hpp
    - cat include/vg_git_version.hpp
    # Build but don't push, just fill the cache
    - docker buildx build --platform=linux/amd64 --build-arg THREADS=8 --target run -f Dockerfile .
    # Run the tests
    - docker buildx build --platform=linux/amd64 --build-arg THREADS=8 --target test -f Dockerfile .
    # Connect so we can upload our images
    - docker login -u "${CI_REGISTRY_USER}" -p "${CI_REGISTRY_PASSWORD}" "${CI_REGISTRY}"
    # Keep trying to push until it works or we time out or run out of tries
    - COUNT=0
    - while ! docker buildx build --platform=linux/amd64 --build-arg THREADS=8 --target run --push -t "quay.io/vgteam/vg:ci-${CI_PIPELINE_IID}-${CI_COMMIT_SHA}" -f Dockerfile . ; do docker logout "${CI_REGISTRY}" ; sleep 30; docker login -u "${CI_REGISTRY_USER}" -p "${CI_REGISTRY_PASSWORD}" "${CI_REGISTRY}" ; sleep 30; ((COUNT+=1)); if [[ ${COUNT} == 10 ]] ; then exit 1; fi; done
  variables:
    GIT_SUBMODULE_STRATEGY: recursive

# The arm container build takes like 90 minutes, so we don't want to run it
# before the main test phase where the other long tests live.
# They also don't pass yet.
arm-build-job:
  stage: test
  only:
    - /^arm/
  before_script:
    # Don't bother starting the Docker daemon or installing much
    - which docker || (sudo apt-get -q -y update && sudo apt-get -q -y install --no-upgrade docker.io)
    # Get buildx
    - mkdir -p ~/.docker/cli-plugins/ ; curl -L https://github.com/docker/buildx/releases/download/v0.5.1/buildx-v0.5.1.linux-amd64 >  ~/.docker/cli-plugins/docker-buildx ; chmod u+x ~/.docker/cli-plugins/docker-buildx
    # Connect to the Kubernetes-based builder "buildkit"
    # See vgci/buildkit-deployment.yml
    - docker buildx create --use --name=buildkit --platform=linux/amd64,linux/arm64 --node=buildkit-amd64 --driver=kubernetes --driver-opt="nodeselector=kubernetes.io/arch=amd64"
    # Prune down build cache to make space
    - docker buildx prune --keep-storage 80G
    # Connect so we can upload our images
    - docker login -u "${CI_REGISTRY_USER}" -p "${CI_REGISTRY_PASSWORD}" "${CI_REGISTRY}"
  script:
    - make include/vg_git_version.hpp
    # Build the arm container (emulated!)
    - docker buildx build --platform=linux/arm64 --build-arg THREADS=8 --target run --push -t "quay.io/vgteam/vg:ci-${CI_PIPELINE_IID}-${CI_COMMIT_SHA}" -f Dockerfile .
    # Also run the tests (emulated!)
    # But don't fail if they fail yet, because they don't yet actually work.
    - docker buildx build --platform=linux/arm64 --build-arg THREADS=8 --target test -f Dockerfile . || true
  variables:
    GIT_SUBMODULE_STRATEGY: recursive

# We also run the toil-vg/pytest-based tests
# Note that WE ONLY RUN TESTS LISTED IN vgci/test-list.txt
test-job:
  stage: test
  # Run in parallel, setting CI_NODE_INDEX and CI_NODE_TOTAL
  # We will find our share of tests from vgci/test-list.txt and run them
  # We ought to run one job per test, but we can wrap around.
  parallel: 6 
  script:
    - docker pull "quay.io/vgteam/vg:ci-${CI_PIPELINE_IID}-${CI_COMMIT_SHA}"
    - docker tag "quay.io/vgteam/vg:ci-${CI_PIPELINE_IID}-${CI_COMMIT_SHA}" vgci-docker-vg-local
    - mkdir -p junit
    # Drop secrets before we do any Toil; it might want to log the environment
    - export GITLAB_SECRET_FILE_AWS_CREDENTIALS=""
    - export GITLAB_SECRET_FILE_DOCS_SSH_KEY=""
    - export CI_REGISTRY_PASSWORD=""
    - export GH_TOKEN=""
    # Make sure IO to Gitlab is in blocking mode so we can't swamp it and crash
    - vgci/blockify.py bash vgci/vgci-parallel-wrapper.sh vgci/test-list.txt vgci-docker-vg-local ${CI_NODE_INDEX} ${CI_NODE_TOTAL} ./junit ./test_output
  artifacts:
    # Let Gitlab see the junit report
    reports:
      junit: junit/*.xml
    paths:
      - junit/*.xml
      - test_output/*
    # Make sure they get artifact'd even if (especially when) the tests fail
    when: always
    expire_in: 3 days

# We have a final job in the last stage to compose an HTML report
report-job:
  stage: report
  # Run this even when the tests fail, because otherwise we won't hear about it.
  # Hopefully if the actual build failed we fail at the docker pull and we don't upload stuff for no version.
  when: always
  # All artifacts from previous stages are available
  script:
    # Get the Docker for version detection
    - docker pull "quay.io/vgteam/vg:ci-${CI_PIPELINE_IID}-${CI_COMMIT_SHA}"
    - docker tag "quay.io/vgteam/vg:ci-${CI_PIPELINE_IID}-${CI_COMMIT_SHA}" vgci-docker-vg-local
    # Collect all the junit files from all the test jobs into one
    - junit-merge -o junit.all.xml junit/*.xml
    # All the test output folder artifacts should automatically merge.
    # Make the report and post it.
    # We still need the Docker for version detection.
    # Make sure IO to Gitlab is in blocking mode so we can't swamp it and crash
    - vgci/blockify.py bash vgci/vgci.sh -J junit.all.xml -T vgci-docker-vg-local -W test_output
    
# We need a separate job to build the Doxygen docs
docs-job:
    stage: build
    script:
      - doc/publish-docs.sh
   
  
